<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://hughxuechen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hughxuechen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-31T15:10:36+00:00</updated><id>https://hughxuechen.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Reflections on Progressive Development, Docker and HPC</title><link href="https://hughxuechen.github.io/blog/2025/docker-HPC/" rel="alternate" type="text/html" title="Reflections on Progressive Development, Docker and HPC"/><published>2025-07-31T00:00:00+00:00</published><updated>2025-07-31T00:00:00+00:00</updated><id>https://hughxuechen.github.io/blog/2025/docker-HPC</id><content type="html" xml:base="https://hughxuechen.github.io/blog/2025/docker-HPC/"><![CDATA[<p>These are my personal reflections on the evolution of development workflows for data science and AI projects. I am summarizing my experience with Docker, WSL, and HPC environments, and how to progressively scale up from local prototyping to large-scale computation.</p> <h1 id="-complete-progressive-development-path">ğŸ“ˆ Complete Progressive Development Path</h1> <p>Starting Point: Docker Template<br/> â†“<br/> Phase 1: Local development on macOS<br/> â†“ (when GPU is needed)<br/> Phase 2: WSL + Linux Docker + CUDA<br/> â†“ (when large-scale computation is needed)<br/> Phase 3: HPC Cluster + Apptainer</p> <h1 id="-detailed-technical-evolution">ğŸ”„ Detailed Technical Evolution</h1> <h2 id="phase-1-local-prototyping-on-macos">Phase 1: Local Prototyping on macOS</h2> <p>docker template â†’ adjust Python packages â†’ run business logic on macOS<br/> docker/run-macos.sh â†’ http://localhost:8888/lab</p> <ul> <li>âœ… Rapid idea validation</li> <li>âœ… Focus on business logic</li> <li>âœ… 5-minute environment setup</li> </ul> <h2 id="phase-2-gpu-support-if-needed">Phase 2: GPU Support (if needed)</h2> <p>macOS â†’ SSH connection â†’ WSL/Linux â†’ Docker + CUDA â†’ GPU acceleration<br/> VS Code Remote-SSH â†’ Windows WSL â†’ docker/run-linux.sh</p> <ul> <li>âœ… Accelerated GPU training</li> <li>âœ… Cross-platform team collaboration</li> <li>âœ… Linux environment compatibility testing</li> </ul> <h2 id="phase-3-large-scale-computation-on-hpc-cluster">Phase 3: Large-Scale Computation on HPC Cluster</h2> <p>macOS â†’ SSH â†’ vera2 â†’ Docker to Apptainer conversion â†’ submit-job.sh â†’ compute node<br/> VS Code â†’ vera2.c3se.chalmers.se â†’ apptainer/ â†’ sbatch â†’ vera-r0x-xx<br/> â†‘ â†“<br/> Port forwarding â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VS Code PORTS panel â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Jupyter Lab</p> <ul> <li>âœ… Access to large-scale computing resources</li> <li>âœ… Long-running job support</li> <li>âœ… Professional HPC environment</li> </ul> <h1 id="-key-transition-points">ğŸ¯ Key Transition Points</h1> <p>Docker â†’ Apptainer Conversion</p> <h1 id="docker-local">Docker (local)</h1> <p>FROM python:3.10<br/> RUN pip install packages<br/> CMD [â€œjupyterâ€, â€œlabâ€]<br/> â†“</p> <h1 id="apptainer-hpc">Apptainer (HPC)</h1> <p>Bootstrap: docker<br/> From: python:3.10<br/> %post<br/> pip install packages<br/> %runscript<br/> exec jupyter lab â€œ$@â€</p> <p>Connection Method Evolution</p> <p>Local: Direct access to localhost:8888<br/> WSL: SSH tunnel macOS â†’ WSL â†’ localhost:8888<br/> HPC: SSH tunnel macOS â†’ vera2 â†’ compute node:random port</p> <h1 id="-technical-stack-evolution-summary">ğŸš€ Technical Stack Evolution Summary</h1> <table> <thead> <tr> <th>Phase</th> <th>Environment</th> <th>Connection Method</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>Phase 1</td> <td>Docker + macOS</td> <td>Direct access</td> <td>Prototyping, small-scale</td> </tr> <tr> <td>Phase 2</td> <td>Docker + WSL/Linux</td> <td>SSH â†’ WSL</td> <td>GPU training, cross-platform collaboration</td> </tr> <tr> <td>Phase 3</td> <td>Apptainer + HPC</td> <td>SSH â†’ cluster</td> <td>Large-scale computation, production deployment</td> </tr> </tbody> </table> <h1 id="-core-value-summary">ğŸ’¡ Core Value Summary</h1> <p>The elegance of this logic:</p> <ol> <li>Progressive complexity: Only increase technical complexity when truly needed</li> <li>Environment consistency: Docker/Apptainer ensures cross-platform code compatibility</li> <li>Connection reuse: macOS serves as the unified development entry point, SSH as the universal connection method</li> <li>Technical debt management: Each phase solves the most important problem at that stage</li> </ol> <h1 id="final-outcomes">Final outcomes:</h1> <ul> <li>ğŸ¯ One codebase, runs in multiple environments</li> <li>ğŸ¯ Unified development experience (all via Jupyter Lab)</li> <li>ğŸ¯ Flexible resource allocation (local â†’ GPU â†’ HPC)</li> <li>ğŸ¯ Optimal cost-effectiveness (upgrade as needed)</li> </ul>]]></content><author><name></name></author><category term="Reddit"/><category term="API"/><summary type="html"><![CDATA[Some reflections on using Reddit data]]></summary></entry><entry><title type="html">Reddit Data</title><link href="https://hughxuechen.github.io/blog/2024/reddit/" rel="alternate" type="text/html" title="Reddit Data"/><published>2024-09-07T00:00:00+00:00</published><updated>2024-09-07T00:00:00+00:00</updated><id>https://hughxuechen.github.io/blog/2024/reddit</id><content type="html" xml:base="https://hughxuechen.github.io/blog/2024/reddit/"><![CDATA[<h4 id="motivation">Motivation</h4> <p>Reddit is a treasure trove of data. It is a platform where users can submit content, comment on posts, and vote on submissions. It is a great source of data for a variety of reasons.</p> <h4 id="reddit-api">Reddit API</h4> <p>There are a lot of tutorials on how to use the Reddit API. I will not go into the details of how to use the API, but I will share some tips that I learned.</p> <p>The general idea is use praw to authenticate and then use the API to get the data. You need to <a href="https://www.reddit.com/prefs/apps">create an app</a> to get the credentials (â€œclient_idâ€ and â€œsecretâ€).</p> <p>The critical problem I met was that Reddit has the rate limit and the date of data is not far from today. The data I got is only about 1k posts which is the latest at the time of my request. My longitudinal study needs more data and the data should be from the past.</p> <h4 id="project-arctic-shift">Project Arctic Shift</h4> <p>Luckily, I found a project called <a href="https://github.com/ArthurHeitmann/arctic_shift">Project Arctic Shift</a> which aims to archive all the posts and comments on Reddit. This Github repository is self-explanatory so I only mention several points that I think are important.</p> <ol> <li>Do not unzip the .zst file since it takes too much storage. Use code to read it and extract the data you need.</li> <li>If you are extracting the data and save in csv format in Python, be careful with the â€œselftextâ€ field. It contains the main text of the post/comment. If it is too long, the file will be corrupted which create break lines in the rows.</li> </ol>]]></content><author><name></name></author><category term="Reddit"/><category term="API"/><summary type="html"><![CDATA[Some reflections on using Reddit data]]></summary></entry></feed>