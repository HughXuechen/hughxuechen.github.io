<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://hughxuechen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hughxuechen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-20T08:49:12+00:00</updated><id>https://hughxuechen.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Summary and Reflection on Fedora</title><link href="https://hughxuechen.github.io/blog/2025/fedora/" rel="alternate" type="text/html" title="Summary and Reflection on Fedora"/><published>2025-08-01T00:00:00+00:00</published><updated>2025-08-01T00:00:00+00:00</updated><id>https://hughxuechen.github.io/blog/2025/fedora</id><content type="html" xml:base="https://hughxuechen.github.io/blog/2025/fedora/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>Currently, my PC (with Nvidia GPU) serves only one purpose: training models using CUDA as the second stage of my progressive development pipeline. In this pipeline, Windows 11 essentially acts as a launcher for WSL 2 to run my Linux-based code. This made me wonder - why not bypass Windows 11 entirely and install a Linux OS directly on the hardware?</p> <h2 id="major-decisions">Major Decisions</h2> <p>Having previously installed macOS on PC laptops and Windows on Intel MacBooks, installing another OS wasn’t daunting.</p> <p>The first decision was choosing which Linux distribution to use. Since I planned to use it as the main system on my PC, I wanted something both functional <strong>and</strong> aesthetic. After browsing options, Elementary OS and Fedora caught my attention. I ultimately chose Fedora for its simplicity, clean interface, and ease of use. I tested both distributions in VMs on my macOS.</p> <p>Next was creating the bootable USB drive. I used BalenaEtcher with the Fedora Workstation Live ISO to create the installation media. This step was quite straightforward.</p> <p>The challenge came when partitioning the hard drive. Windows’ default disk management tool failed to provide enough space. After debugging for a full day without success, I tried AOMEI, but its free version didn’t support partition adjustment. Finally, MiniTool Partition Wizard solved the problem.</p> <p>During the Fedora installation, I initially forgot to disable Secure Boot in BIOS. Once I addressed that, the installation process was straightforward.</p> <p>I needed to remap keys since I’m more familiar with the macOS layout, which I accomplished using GNOME Tweaks. For Chinese input, I had to configure additional settings to use <code class="language-plaintext highlighter-rouge">[]</code> for navigating between Chinese character pages.</p> <p>Finally, I installed my development environment, including VS Code, Docker, CUDA drivers, Claude Code, and other essential tools.</p> <p>Exploring the new world!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Summary and Reflection on installing Fedora as the main PC OS]]></summary></entry><entry><title type="html">Reflections on Progressive Development, Docker and HPC</title><link href="https://hughxuechen.github.io/blog/2025/docker-HPC/" rel="alternate" type="text/html" title="Reflections on Progressive Development, Docker and HPC"/><published>2025-07-31T00:00:00+00:00</published><updated>2025-07-31T00:00:00+00:00</updated><id>https://hughxuechen.github.io/blog/2025/docker-HPC</id><content type="html" xml:base="https://hughxuechen.github.io/blog/2025/docker-HPC/"><![CDATA[<p>These are my personal reflections on the evolution of development workflows for data science and AI projects. I am summarizing my experience with Docker, WSL, and HPC environments, and how to progressively scale up from local prototyping to large-scale computation.</p> <h2 id="complete-progressive-development-path">Complete Progressive Development Path</h2> <p>Starting Point: Docker Template<br/> ↓<br/> Phase 1: Local development on macOS<br/> ↓ (when GPU is needed)<br/> Phase 2: WSL + Linux Docker + CUDA<br/> ↓ (when large-scale computation is needed)<br/> Phase 3: HPC Cluster + Apptainer</p> <h2 id="detailed-technical-evolution">Detailed Technical Evolution</h2> <h3 id="phase-1-local-prototyping-on-macos">Phase 1: Local Prototyping on macOS</h3> <p>docker template → adjust Python packages → run business logic on macOS<br/> docker/run-macos.sh → http://localhost:8888/lab</p> <ul> <li>Rapid idea validation</li> <li>Focus on business logic</li> <li>5-minute environment setup</li> </ul> <h3 id="phase-2-gpu-support-if-needed">Phase 2: GPU Support (if needed)</h3> <p>macOS → SSH connection → WSL/Linux → Docker + CUDA → GPU acceleration<br/> VS Code Remote-SSH → Windows WSL → docker/run-linux.sh</p> <ul> <li>Accelerated GPU training</li> <li>Cross-platform team collaboration</li> <li>Linux environment compatibility testing</li> </ul> <h3 id="phase-3-large-scale-computation-on-hpc-cluster">Phase 3: Large-Scale Computation on HPC Cluster</h3> <p>macOS → SSH → vera2 → Docker to Apptainer conversion → submit-job.sh → compute node<br/> VS Code → vera2.c3se.chalmers.se → apptainer/ → sbatch → vera-r0x-xx<br/> ↑ ↓<br/> Port forwarding ←────────── VS Code PORTS panel ←──────────── Jupyter Lab</p> <ul> <li>Access to large-scale computing resources</li> <li>Long-running job support</li> <li>Professional HPC environment</li> </ul> <h2 id="key-transition-points">Key Transition Points</h2> <p>Docker → Apptainer Conversion</p> <h2 id="docker-local">Docker (local)</h2> <p>FROM python:3.10<br/> RUN pip install packages<br/> CMD [“jupyter”, “lab”]<br/> ↓</p> <h2 id="apptainer-hpc">Apptainer (HPC)</h2> <p>Bootstrap: docker<br/> From: python:3.10<br/> %post<br/> pip install packages<br/> %runscript<br/> exec jupyter lab “$@”</p> <p>Connection Method Evolution</p> <p>Local: Direct access to localhost:8888<br/> WSL: SSH tunnel macOS → WSL → localhost:8888<br/> HPC: SSH tunnel macOS → vera2 → compute node:random port</p> <h2 id="technical-stack-evolution-summary">Technical Stack Evolution Summary</h2> <table> <thead> <tr> <th>Phase</th> <th>Environment</th> <th>Connection Method</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>Phase 1</td> <td>Docker + macOS</td> <td>Direct access</td> <td>Prototyping, small-scale</td> </tr> <tr> <td>Phase 2</td> <td>Docker + WSL/Linux</td> <td>SSH → WSL</td> <td>GPU training, cross-platform collaboration</td> </tr> <tr> <td>Phase 3</td> <td>Apptainer + HPC</td> <td>SSH → cluster</td> <td>Large-scale computation, production deployment</td> </tr> </tbody> </table> <h2 id="core-value-summary">Core Value Summary</h2> <p>The elegance of this logic:</p> <ol> <li>Progressive complexity: Only increase technical complexity when truly needed</li> <li>Environment consistency: Docker/Apptainer ensures cross-platform code compatibility</li> <li>Connection reuse: macOS serves as the unified development entry point, SSH as the universal connection method</li> <li>Technical debt management: Each phase solves the most important problem at that stage</li> </ol> <h2 id="final-outcomes">Final outcomes:</h2> <ul> <li>One codebase, runs in multiple environments</li> <li>Unified development experience (all via Jupyter Lab)</li> <li>Flexible resource allocation (local → GPU → HPC)</li> <li>Optimal cost-effectiveness (upgrade as needed)</li> </ul>]]></content><author><name></name></author><category term="Reddit"/><category term="API"/><summary type="html"><![CDATA[Some reflections on using Reddit data]]></summary></entry><entry><title type="html">Reddit Data</title><link href="https://hughxuechen.github.io/blog/2024/reddit/" rel="alternate" type="text/html" title="Reddit Data"/><published>2024-09-07T00:00:00+00:00</published><updated>2024-09-07T00:00:00+00:00</updated><id>https://hughxuechen.github.io/blog/2024/reddit</id><content type="html" xml:base="https://hughxuechen.github.io/blog/2024/reddit/"><![CDATA[<h4 id="motivation">Motivation</h4> <p>Reddit is a treasure trove of data. It is a platform where users can submit content, comment on posts, and vote on submissions. It is a great source of data for a variety of reasons.</p> <h4 id="reddit-api">Reddit API</h4> <p>There are a lot of tutorials on how to use the Reddit API. I will not go into the details of how to use the API, but I will share some tips that I learned.</p> <p>The general idea is use praw to authenticate and then use the API to get the data. You need to <a href="https://www.reddit.com/prefs/apps">create an app</a> to get the credentials (“client_id” and “secret”).</p> <p>The critical problem I met was that Reddit has the rate limit and the date of data is not far from today. The data I got is only about 1k posts which is the latest at the time of my request. My longitudinal study needs more data and the data should be from the past.</p> <h4 id="project-arctic-shift">Project Arctic Shift</h4> <p>Luckily, I found a project called <a href="https://github.com/ArthurHeitmann/arctic_shift">Project Arctic Shift</a> which aims to archive all the posts and comments on Reddit. This Github repository is self-explanatory so I only mention several points that I think are important.</p> <ol> <li>Do not unzip the .zst file since it takes too much storage. Use code to read it and extract the data you need.</li> <li>If you are extracting the data and save in csv format in Python, be careful with the “selftext” field. It contains the main text of the post/comment. If it is too long, the file will be corrupted which create break lines in the rows.</li> </ol>]]></content><author><name></name></author><category term="Reddit"/><category term="API"/><summary type="html"><![CDATA[Some reflections on using Reddit data]]></summary></entry></feed>